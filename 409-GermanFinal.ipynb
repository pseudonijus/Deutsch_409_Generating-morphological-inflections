{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "569b7055-4640-42f5-ba19-b9c4dd2be5fc",
   "metadata": {
    "panel-layout": {
     "height": 93.5625,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "# German Group Final\n",
    "LING 409 Fall 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe12f7c1-bdb7-4d96-a1f6-d59e8619f56d",
   "metadata": {
    "panel-layout": {
     "height": 507.0250244140625,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "### Enhancing a Non-Neural Morphological Model for German Separable Verbs\n",
    "\n",
    "This notebook provides a runnable implementation of a non-neural morphological model, complete with function documentation, descriptions of tweaks made for handling separable verbs, and evaluation results.\n",
    "\n",
    "## Background\n",
    "\n",
    "### Morphological Inflection and Separable Verbs\n",
    "German separable verbs consist of a **prefix** (e.g., `ab`, `auf`) and a **root** (e.g., `haken`, `stehen`). These verbs behave uniquely in certain grammatical contexts:\n",
    "- The prefix moves to the end of the clause.\n",
    "- Example:\n",
    "    - Lemma: `abhaken` (to check off)\n",
    "    - Inflected Form: `hake ab`\n",
    "\n",
    "### Problem\n",
    "The baseline model struggled with separable verbs because:\n",
    "1. Prefixes and roots were not handled independently.\n",
    "2. Prefix rules were not extracted or applied.\n",
    "\n",
    "Our goal was to improve the handling of separable verbs by:\n",
    "1. Detecting separable prefixes in the lemma.\n",
    "2. Extracting transformation rules for prefixes and roots separately.\n",
    "3. Applying prefix and suffix rules independently during prediction."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e69f9488-5868-411d-8bc9-dcb2c51dfeb8",
   "metadata": {},
   "source": [
    "### Imports and Utility Functions\n",
    "sys and os: Useful for system-level operations like file paths and arguments.\n",
    "getopt: For parsing command-line options and arguments.\n",
    "re: For working with regular expressions.\n",
    "functools.wraps: A decorator helper that preserves metadata of wrapped functions.\n",
    "glob: For finding files matching a specific pattern, often useful for processing multiple files in a directory."
   ]
  },
  {
   "cell_type": "raw",
   "id": "746cdb42-6f6a-44ee-9a12-4d98e8a954e3",
   "metadata": {},
   "source": [
    "import sys, os, getopt, re\n",
    "#sys, os: system level operations (file paths, arguments)\n",
    "#getopt: parsing command-line options\n",
    "#re: regular expressions\n",
    "from functools import wraps\n",
    "#preserves wrapped functions\n",
    "from glob import glob\n",
    "#globbing (method for finding files with similar naming schema)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96fb59d2-9feb-46c1-9d82-81e120d53e51",
   "metadata": {},
   "source": [
    "### Hamming Distance\n",
    "This function calculates the Hamming distance between two strings, s and t. The Hamming distance measures the number of positions where the characters in the two strings differ. Here's a breakdown:\n",
    "\n",
    "zip(s, t): Pairs corresponding characters from the two strings.\n",
    "if x != y: Checks if the characters in each position are different.\n",
    "sum(1 for ...): Counts how many positions differ and returns that count."
   ]
  },
  {
   "cell_type": "raw",
   "id": "de16c1a1-9162-4478-96da-e7e64d6fad06",
   "metadata": {},
   "source": [
    "def hamming(s,t):\n",
    "    \"\"\"Calculate the Hamming distance between two given strings, s, and t:\n",
    "    (ie: how many substitutions needed to turn one string into another).\"\"\"\n",
    "    return sum(1 for x,y in zip(s,t) if x != y)\n",
    "    #iterates through each character in the zipped pair; for x,y in zip(s,t)\n",
    "    #if x and y are not equal; if x!=y, it adds 1 to the total pair; sum()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2ffeb23-833d-44ac-9ad3-d43ae8c676a4",
   "metadata": {},
   "source": [
    "### Align by Hamming Distance\n",
    "The halign function aligns two strings, s and t, by inserting underscores (_) into one string to minimize the Hamming distance between the two."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b37994e2-50b9-4a43-9216-cda5887c0fc7",
   "metadata": {},
   "source": [
    "def halign(s,t):\n",
    "    \"\"\"Align two strings by Hamming distance by padding them with\n",
    "    underscores so that their Hamming distance can be minimized.\"\"\"\n",
    "    slen = len(s) #stores the length of string s\n",
    "    tlen = len(t) #stores the length of string t\n",
    "    minscore = len(s) + len(t) + 1\n",
    "    #minscore keeps track of the current minimum Hamming distance\n",
    "    #initialized as the sum of the string lengths plus one\n",
    "    for upad in range(0, len(t)+1): #padding s to align with t\n",
    "        upper = '_' * upad + s + (len(t) - upad) * '_'\n",
    "        #pads s with 'upad' underscores on the left  and 'len(t)-upad) underscores on the right\n",
    "        lower = len(s) * '_' + t\n",
    "        #pads t with len(s) underscores to match the length of upper\n",
    "        score = hamming(upper, lower)\n",
    "        #calculates the Hamming distance with current padding\n",
    "        if score < minscore: #if Hamming distance is less than current minscore\n",
    "            bu = upper #stores padded version of s for current minimum\n",
    "            bl = lower #stores padded version of t for current minimum\n",
    "            minscore = score #updates with current minimum Hamming distance\n",
    "\n",
    "    for lpad in range(0, len(s)+1): #same as above but for string t\n",
    "        upper = len(t) * '_' + s\n",
    "        lower = (len(s) - lpad) * '_' + t + '_' * lpad\n",
    "        score = hamming(upper, lower)\n",
    "        if score < minscore:\n",
    "            bu = upper\n",
    "            bl = lower\n",
    "            minscore = score\n",
    "    zipped = list(zip(bu,bl)) #zips padded versions of s and t for minimum Hamming distance\n",
    "    newin  = ''.join(i for i,o in zipped if i != '_' or o != '_') #stores the version of s without the underscores\n",
    "    newout = ''.join(o for i,o in zipped if i != '_' or o != '_') #stores the version of t without the underscores\n",
    "    return newin, newout"
   ]
  },
  {
   "cell_type": "raw",
   "id": "66fdecf8-6b09-4984-b858-c8676718b0e8",
   "metadata": {},
   "source": [
    "### Levenshtein Distance\n",
    "This function, levenshtein, computes the Levenshtein distance between two strings, s and t, using a recursive approach. The Levenshtein distance measures the minimum number of single-character edits (insertions, deletions, or substitutions) required to transform one string into another."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4be1718d-8e07-4e9d-b907-9567db6994bd",
   "metadata": {},
   "source": [
    "def levenshtein(s, t, inscost = 1.0, delcost = 1.0, substcost = 1.0):\n",
    "    \"\"\"Recursive implementation of Levenshtein, with alignments returned by\n",
    "    calculating the minimum number of edits (insert, delete, substitute) to transform s to t.\"\"\"\n",
    "    #s and t: input strings to compare\n",
    "    #inscost, delcost, substcost: costs of insertion, deletion, and substitution, respectively (default: 1.0)\n",
    "    @memolrec\n",
    "    #memoizer for lrec function (as seen below)\n",
    "    def lrec(spast, tpast, srem, trem, cost):\n",
    "        if len(srem) == 0:\n",
    "            #base case of recursive algorithm: if there are no more remaining characters in s\n",
    "            return spast + len(trem) * '_', tpast + trem, '', '', cost + len(trem)\n",
    "        if len(trem) == 0:\n",
    "            return spast + srem, tpast + len(srem) * '_', '', '', cost + len(srem)\n",
    "            #base case of recursive algorithm: if there are no more remaining characters in t\n",
    "\n",
    "        addcost = 0\n",
    "        #total cost of insertions/deletions/substitutions, initialized at 0\n",
    "        if srem[0] != trem[0]:\n",
    "            addcost = substcost\n",
    "            #checks if first characters of s and t are not equal, if so, add cost of substition to total cost\n",
    "\n",
    "        return min((lrec(spast + srem[0], tpast + trem[0], srem[1:], trem[1:], cost + addcost),\n",
    "                    #substition\n",
    "                    #adds first character of srem to spast, removes from srem\n",
    "                    #adds first character of trem to tpast, removes from trem\n",
    "                   lrec(spast + '_', tpast + trem[0], srem, trem[1:], cost + inscost),\n",
    "                    #insertion\n",
    "                    #adds underscore to spast to represent an insertion\n",
    "                    #add first character of trem to tpast, removes from trem\n",
    "                   lrec(spast + srem[0], tpast + '_', srem[1:], trem, cost + delcost)),\n",
    "                    #deletion\n",
    "                    #adds first character of srem to spast, removes from srem\n",
    "                    #adds underscore to tpast to represent an deletion\n",
    "                   key = lambda x: x[4])\n",
    "                    #finds the minimum cost using minimum number of substitutions, insertions, deletions\n",
    "\n",
    "    answer = lrec('', '', s, t, 0)\n",
    "    #spast and tpast are initialized as empty strings as no characters have been processed yet\n",
    "    #srem and trem are initialized as the full strings as they have yet to be processed\n",
    "    #cost initialized to 0\n",
    "    return answer[0],answer[1],answer[4]\n",
    "\n",
    "\n",
    "def memolrec(func):\n",
    "    \"\"\"Memoizer for Levenshtein (cache to check if Levenshtein distance between s and t has already been calculated\n",
    "    before to save time, as Levenshtein is a recursive algorithm that repeatedly solves subproblems.\"\"\"\n",
    "    cache = {}\n",
    "    @wraps(func)\n",
    "    def wrap(sp, tp, sr, tr, cost):\n",
    "        if (sr,tr) not in cache:\n",
    "            res = func(sp, tp, sr, tr, cost)\n",
    "            cache[(sr,tr)] = (res[0][len(sp):], res[1][len(tp):], res[4] - cost)\n",
    "        return sp + cache[(sr,tr)][0], tp + cache[(sr,tr)][1], '', '', cost + cache[(sr,tr)][2]\n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631e243-b5df-407a-ab72-b8662609c8da",
   "metadata": {
    "panel-layout": {
     "height": 95.3375015258789,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "### Alignment\n",
    "The alignprs function takes a lemma (base form of a word) and a form (inflected form) and aligns them to break each into three parts: a prefix, a stem, and a suffix. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a1e755c-8cb1-4282-8b9b-3518b2d282ed",
   "metadata": {},
   "source": [
    "def alignprs(lemma, form):\n",
    "    \"\"\"Align lemma and form into prefix, stem, and suffix.\"\"\"\n",
    "    alemma, aform = levenshtein('', '', lemma, form, 0)[:2]\n",
    "    lspace = max(len(alemma) - len(alemma.lstrip('_')), len(aform) - len(aform.lstrip('_')))\n",
    "    tspace = max(len(alemma[::-1]) - len(alemma[::-1].lstrip('_')), len(aform[::-1]) - len(aform[::-1].lstrip('_')))\n",
    "    return (alemma[:lspace], alemma[lspace:-tspace], alemma[-tspace:],\n",
    "            aform[:lspace], aform[lspace:-tspace], aform[-tspace:])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6aaa250f-429e-4775-a924-457935f054ae",
   "metadata": {},
   "source": [
    "### List of separable prefixes \n",
    "Defined a list of common German separable prefixes (SEPARABLE_PREFIXES), such as ab, auf, an, etc."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c721c2ca-51c1-4748-89a2-90b7c054c3a2",
   "metadata": {},
   "source": [
    "SEPARABLE_PREFIXES = {\"ab\", \"an\", \"auf\", \"aus\", \"bei\", \"ein\", \"mit\", \"nach\", \"vor\", \"weg\", \"zu\"} #EDIT"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c59b507-a61b-4005-8525-e9ebe93fadeb",
   "metadata": {},
   "source": [
    "### Extracting prefix and suffix rules\n",
    "This function analyzes how the prefix and suffix of a lemma change when it's transformed into an inflected form. The rules generated are for morphological processing, enabling a system to predict transformations for other words.\n",
    "### Tweaking rule extraction for prefixes\n",
    "Identifies if lemma contains a separable prefix, in order to disolate the prefix from the lemma for separate processing when generating rules."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b264a03d-55d3-4ac2-ad7e-7068af23de0c",
   "metadata": {},
   "source": [
    "def prefix_suffix_rules_get(lemma, form):\n",
    "    \"\"\"Extract a number of suffix-change and prefix-change rules\n",
    "    based on a given example lemma+inflected form. Analyze how the prefix and suffix change given a lemma and inflected form in order\n",
    "    to predict forms for other words.\"\"\" \n",
    "    prefix = ''\n",
    "    root = lemma\n",
    "    for p in SEPARABLE_PREFIXES:\n",
    "        if lemma.startswith(p):\n",
    "            prefix = p\n",
    "            root = lemma[len(p):] #removes prefix to isolate root\n",
    "            break\n",
    "    lp,lr,ls,fp,fr,fs = alignprs(root, form) #aligns root with lemma, get six parts, three for in three for out\n",
    "    #lp, lr, ls: lemma prefix, lemma root, lemma suffix\n",
    "    #fp, fr, fs: form prefix, form root, form suffix\n",
    "\n",
    "    #Prefix rules\n",
    "    prules = set()\n",
    "    if prefix:\n",
    "        prules.add((prefix, fp)) #rule for transforming the prefix\n",
    "\n",
    "    # Suffix rules\n",
    "    ins  = lr + ls + \">\"\n",
    "    #stem and suffix of lemma, '>' indicates end\n",
    "    outs = fr + fs + \">\"\n",
    "    #stem and suffix of form, '>' indicates end\n",
    "    srules = set()\n",
    "    #initializes empty set for suffix rules\n",
    "    for i in range(min(len(ins), len(outs))):\n",
    "        #iterates through either the stem+suffix of lemma or stem+suffix of form, whichever is shorter\n",
    "        srules.add((ins[i:], outs[i:]))\n",
    "        #adds ins and outs sliced from index i to end into set of suffix rules\n",
    "    srules = {(x[0].replace('_',''), x[1].replace('_','')) for x in srules}\n",
    "    #removes underscores from suffix rules by replacing with empty strings\n",
    "\n",
    "    return prules, srules\n",
    "    #returns set of prefix and suffix rules, respectively"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a90a41d9-d47d-48db-8ba7-02306b1ed1dc",
   "metadata": {},
   "source": [
    "### Applying Rules\n",
    "The apply_best_rule function applies the best-fitting prefix and suffix transformation rules to a given lemma, using morphological and syntactic descriptions (MSD) to guide the selection.\n",
    "\n",
    "### Adding logic for prefix rules\n",
    "We added logic to apply prefix rules that moved the prefix to the appropriate position (e.g., abhaken â†’ haken ab), while keeping suffix rules the same."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7c27b6a-d6ef-481c-9b1b-8baf3815362e",
   "metadata": {},
   "source": [
    "def apply_best_rule(lemma, msd, allprules, allsrules):\n",
    "    \"\"\"Applies the longest-matching suffix-changing rule given an input\n",
    "    form and the MSD. Length ties in suffix rules are broken by frequency.\n",
    "    For prefix-changing rules, only the most frequent rule is chosen.\"\"\"\n",
    "    #lemma: base\n",
    "    #msd: morphosyntactic description (eg. past, plural, etc.)\n",
    "    #allprules: dictionary of prefix rules grouped by msd\n",
    "    #allsrules: dictionary of suffix rules grouped by msd\n",
    "\n",
    "    bestrulelen = 0\n",
    "    base = \"<\" + lemma + \">\"\n",
    "    #wrap lemma with '<' and '>' to match rule format\n",
    "    if msd not in allprules and msd not in allsrules:\n",
    "        return lemma #haven't seen this inflection, so bail out\n",
    "\n",
    "    if msd in allsrules:\n",
    "        applicablerules = [(x[0],x[1],y) for x,y in allsrules[msd].items() if x[0] in base]\n",
    "        if applicablerules:\n",
    "            bestrule = max(applicablerules, key = lambda x: (len(x[0]), x[2], len(x[1])))\n",
    "            base = base.replace(bestrule[0], bestrule[1])\n",
    "\n",
    "    if msd in allprules:\n",
    "        prefix = next((p for p in SEPARABLE_PREFIXES if base.startswith(\"<\" + p)), \"\")\n",
    "        if prefix:\n",
    "            applicablerules = [(x[0], x[1], y) for x, y in allprules[msd].items() if x[0] == prefix]\n",
    "            if applicablerules:\n",
    "                bestrule = max(applicablerules, key=lambda x: x[2])  # Choose most frequent rule\n",
    "                root = lemma[len(prefix):]  # Remove the original prefix\n",
    "                print(f\"Original Lemma: {lemma}, Prefix Detected: {prefix}\")\n",
    "                print(f\"Root: {root}\") #EDIT\n",
    "                lemma = f\"{bestrule[1]} {lemma}\"  # Add the transformed prefix at the correct position\n",
    "                #base = base.replace(bestrule[0], bestrule[1], 1)  # Replace prefix\n",
    "    \n",
    "    base = base.replace('<', '')\n",
    "    base = base.replace('>', '')\n",
    "    return base\n",
    "\n",
    "def numleadingsyms(s, symbol):\n",
    "    return len(s) - len(s.lstrip(symbol))\n",
    "    #returns count of leading occurrences of symbol\n",
    "\n",
    "\n",
    "def numtrailingsyms(s, symbol):\n",
    "    return len(s) - len(s.rstrip(symbol))\n",
    "    #returns count of trailing occurrences of symbol"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5d6708e-378c-42ff-976d-bbfac1bcba03",
   "metadata": {},
   "source": [
    "def main(argv):\n",
    "    options, remainder = getopt.gnu_getopt(argv[1:], 'ohp:', ['output','help','path='])\n",
    "    TEST, OUTPUT, HELP, path = False,False, False, '../data/'\n",
    "    for opt, arg in options:\n",
    "        if opt in ('-o', '--output'):\n",
    "            OUTPUT = True\n",
    "        if opt in ('-t', '--test'):\n",
    "            TEST = True\n",
    "        if opt in ('-h', '--help'):\n",
    "            HELP = True\n",
    "        if opt in ('-p', '--path'):\n",
    "            path = arg\n",
    "\n",
    "    if HELP:\n",
    "            print(\"\\n*** Baseline for the SIGMORPHON 2020 shared task ***\\n\")\n",
    "            print(\"By default, the program runs all languages only evaluating accuracy.\")\n",
    "            print(\"To create output files, use -o\")\n",
    "            print(\"The training and dev-data are assumed to live in ./part1/development_languages/\")\n",
    "            print(\"Options:\")\n",
    "            print(\" -o         create output files with guesses (and don't just evaluate)\")\n",
    "            print(\" -t         evaluate on test instead of dev\")\n",
    "            print(\" -p [path]  data files path. Default is ../data/\")\n",
    "            quit()\n",
    "\n",
    "    totalavg, numlang = 0.0, 0\n",
    "    for lang in [os.path.splitext(d)[0] for d in os.listdir(path) if '.trn' in d]:\n",
    "        allprules, allsrules = {}, {}\n",
    "        if not os.path.isfile(path + lang +  \".trn\"):\n",
    "            continue\n",
    "        lines = [line.strip() for line in open(path + lang + \".trn\", \"r\", encoding='utf8') if line != '\\n']\n",
    "\n",
    "        # First, test if language is predominantly suffixing or prefixing\n",
    "        # If prefixing, work with reversed strings\n",
    "        prefbias, suffbias = 0,0\n",
    "        for l in lines:\n",
    "            lemma, _, form = l.split(u'\\t')\n",
    "            aligned = halign(lemma, form)\n",
    "            if ' ' not in aligned[0] and ' ' not in aligned[1] and '-' not in aligned[0] and '-' not in aligned[1]:\n",
    "                prefbias += numleadingsyms(aligned[0],'_') + numleadingsyms(aligned[1],'_')\n",
    "                suffbias += numtrailingsyms(aligned[0],'_') + numtrailingsyms(aligned[1],'_')\n",
    "        for l in lines: # Read in lines and extract transformation rules from pairs\n",
    "            lemma, msd, form = l.split(u'\\t')\n",
    "            if prefbias > suffbias:\n",
    "                lemma = lemma[::-1]\n",
    "                form = form[::-1]\n",
    "            prules, srules = prefix_suffix_rules_get(lemma, form)\n",
    "\n",
    "            if msd not in allprules and len(prules) > 0:\n",
    "                allprules[msd] = {}\n",
    "            if msd not in allsrules and len(srules) > 0:\n",
    "                allsrules[msd] = {}\n",
    "\n",
    "            for r in prules:\n",
    "                if (r[0],r[1]) in allprules[msd]:\n",
    "                    allprules[msd][(r[0],r[1])] = allprules[msd][(r[0],r[1])] + 1\n",
    "                else:\n",
    "                    allprules[msd][(r[0],r[1])] = 1\n",
    "\n",
    "            for r in srules:\n",
    "                if (r[0],r[1]) in allsrules[msd]:\n",
    "                    allsrules[msd][(r[0],r[1])] = allsrules[msd][(r[0],r[1])] + 1\n",
    "                else:\n",
    "                    allsrules[msd][(r[0],r[1])] = 1\n",
    "\n",
    "        # Run eval on dev\n",
    "        devlines = [line.strip() for line in open(path + lang + \".dev\", \"r\", encoding='utf8') if line != '\\n']\n",
    "        if TEST:\n",
    "            devlines = [line.strip() for line in open(path + lang + \".tst\", \"r\", encoding='utf8') if line != '\\n']\n",
    "        numcorrect = 0\n",
    "        numguesses = 0\n",
    "        if OUTPUT:\n",
    "            outfile = open(path + lang + \".out\", \"w\", encoding='utf8')\n",
    "        for l in devlines:\n",
    "            lemma, msd, correct = l.split(u'\\t')\n",
    "#                    lemma, msd, = l.split(u'\\t')\n",
    "            if prefbias > suffbias:\n",
    "                lemma = lemma[::-1]\n",
    "            outform = apply_best_rule(lemma, msd, allprules, allsrules)\n",
    "            if prefbias > suffbias:\n",
    "                outform = outform[::-1]\n",
    "                lemma = lemma[::-1]\n",
    "            if outform == correct:\n",
    "                numcorrect += 1\n",
    "            numguesses += 1\n",
    "            if OUTPUT:\n",
    "                outfile.write(lemma + \"\\t\" + msd + \"\\t\" + outform + \"\\n\")\n",
    "\n",
    "        if OUTPUT:\n",
    "            outfile.close()\n",
    "\n",
    "        numlang += 1\n",
    "        totalavg += numcorrect/float(numguesses)\n",
    "\n",
    "        print(lang + \": \" + str(str(numcorrect/float(numguesses)))[0:7])\n",
    "\n",
    "    print(\"Average accuracy\", totalavg/float(numlang))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ef87e78-fb1a-4897-8856-f480ab5643e0",
   "metadata": {},
   "source": [
    "### Results\n",
    "| Dataset       | Accuracy Before Tweaks | Accuracy After Tweaks |\n",
    "|---------------|-------------------------|------------------------|\n",
    "| Training Set  | 93.75%                 | 97.93%                |\n",
    "| Development   | 74.5%                  | 76.3%                 |\n",
    "\n",
    "### Observations\n",
    "- Improvements in prefix handling significantly boosted accuracy in the training set, though less so in the dev set.\n",
    "- Prefixes were being correctly moved, but not being removed from the beginning of the verb\n",
    "\n",
    "- Handling separable verbs reduced common alignment errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "panel-cell-order": [
   "569b7055-4640-42f5-ba19-b9c4dd2be5fc",
   "fe12f7c1-bdb7-4d96-a1f6-d59e8619f56d",
   "b631e243-b5df-407a-ab72-b8662609c8da"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
