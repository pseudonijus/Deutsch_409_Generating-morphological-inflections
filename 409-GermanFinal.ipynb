{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "569b7055-4640-42f5-ba19-b9c4dd2be5fc",
   "metadata": {
    "panel-layout": {
     "height": 93.5625,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "# German Group Final\n",
    "LING 409 Fall 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe12f7c1-bdb7-4d96-a1f6-d59e8619f56d",
   "metadata": {
    "panel-layout": {
     "height": 507.0250244140625,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "### Enhancing a Non-Neural Morphological Model for German Separable Verbs\n",
    "\n",
    "This notebook provides a runnable implementation of a non-neural morphological model, complete with function documentation, descriptions of tweaks made for handling separable verbs, and evaluation results.\n",
    "\n",
    "## Background\n",
    "\n",
    "### Morphological Inflection and Separable Verbs\n",
    "German separable verbs consist of a **prefix** (e.g., `ab`, `auf`) and a **root** (e.g., `haken`, `stehen`). These verbs behave uniquely in certain grammatical contexts:\n",
    "- The prefix moves to the end of the clause.\n",
    "- Example:\n",
    "    - Lemma: `abhaken` (to check off)\n",
    "    - Inflected Form: `hake ab`\n",
    "\n",
    "### Problem\n",
    "The baseline model struggled with separable verbs because:\n",
    "1. Prefixes and roots were not handled independently.\n",
    "2. Prefix rules were not extracted or applied.\n",
    "\n",
    "Our goal was to improve the handling of separable verbs by:\n",
    "1. Detecting separable prefixes in the lemma.\n",
    "2. Extracting transformation rules for prefixes and roots separately.\n",
    "3. Applying prefix and suffix rules independently during prediction."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e69f9488-5868-411d-8bc9-dcb2c51dfeb8",
   "metadata": {},
   "source": [
    "### Imports and Utility Functions\n",
    "sys and os: Useful for system-level operations like file paths and arguments.\n",
    "getopt: For parsing command-line options and arguments.\n",
    "re: For working with regular expressions.\n",
    "functools.wraps: A decorator helper that preserves metadata of wrapped functions.\n",
    "glob: For finding files matching a specific pattern, often useful for processing multiple files in a directory."
   ]
  },
  {
   "cell_type": "raw",
   "id": "746cdb42-6f6a-44ee-9a12-4d98e8a954e3",
   "metadata": {},
   "source": [
    "import sys, os, getopt, re\n",
    "#sys, os: system level operations (file paths, arguments)\n",
    "#getopt: parsing command-line options\n",
    "#re: regular expressions\n",
    "from functools import wraps\n",
    "#preserves wrapped functions\n",
    "from glob import glob\n",
    "#globbing (method for finding files with similar naming schema)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96fb59d2-9feb-46c1-9d82-81e120d53e51",
   "metadata": {},
   "source": [
    "### Hamming Distance\n",
    "This function calculates the Hamming distance between two strings, s and t. The Hamming distance measures the number of positions where the characters in the two strings differ. Here's a breakdown:\n",
    "\n",
    "zip(s, t): Pairs corresponding characters from the two strings.\n",
    "if x != y: Checks if the characters in each position are different.\n",
    "sum(1 for ...): Counts how many positions differ and returns that count."
   ]
  },
  {
   "cell_type": "raw",
   "id": "de16c1a1-9162-4478-96da-e7e64d6fad06",
   "metadata": {},
   "source": [
    "def hamming(s,t):\n",
    "    \"\"\"Calculate the Hamming distance between two given strings, s, and t:\n",
    "    (ie: how many substitutions needed to turn one string into another).\"\"\"\n",
    "    return sum(1 for x,y in zip(s,t) if x != y)\n",
    "    #iterates through each character in the zipped pair; for x,y in zip(s,t)\n",
    "    #if x and y are not equal; if x!=y, it adds 1 to the total pair; sum()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2ffeb23-833d-44ac-9ad3-d43ae8c676a4",
   "metadata": {},
   "source": [
    "### Align by Hamming Distance\n",
    "The halign function aligns two strings, s and t, by inserting underscores (_) into one string to minimize the Hamming distance between the two."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b37994e2-50b9-4a43-9216-cda5887c0fc7",
   "metadata": {},
   "source": [
    "def halign(s,t):\n",
    "    \"\"\"Align two strings by Hamming distance by padding them with\n",
    "    underscores so that their Hamming distance can be minimized.\"\"\"\n",
    "    slen = len(s) #stores the length of string s\n",
    "    tlen = len(t) #stores the length of string t\n",
    "    minscore = len(s) + len(t) + 1\n",
    "    #minscore keeps track of the current minimum Hamming distance\n",
    "    #initialized as the sum of the string lengths plus one\n",
    "    for upad in range(0, len(t)+1): #padding s to align with t\n",
    "        upper = '_' * upad + s + (len(t) - upad) * '_'\n",
    "        #pads s with 'upad' underscores on the left  and 'len(t)-upad) underscores on the right\n",
    "        lower = len(s) * '_' + t\n",
    "        #pads t with len(s) underscores to match the length of upper\n",
    "        score = hamming(upper, lower)\n",
    "        #calculates the Hamming distance with current padding\n",
    "        if score < minscore: #if Hamming distance is less than current minscore\n",
    "            bu = upper #stores padded version of s for current minimum\n",
    "            bl = lower #stores padded version of t for current minimum\n",
    "            minscore = score #updates with current minimum Hamming distance\n",
    "\n",
    "    for lpad in range(0, len(s)+1): #same as above but for string t\n",
    "        upper = len(t) * '_' + s\n",
    "        lower = (len(s) - lpad) * '_' + t + '_' * lpad\n",
    "        score = hamming(upper, lower)\n",
    "        if score < minscore:\n",
    "            bu = upper\n",
    "            bl = lower\n",
    "            minscore = score\n",
    "    zipped = list(zip(bu,bl)) #zips padded versions of s and t for minimum Hamming distance\n",
    "    newin  = ''.join(i for i,o in zipped if i != '_' or o != '_') #stores the version of s without the underscores\n",
    "    newout = ''.join(o for i,o in zipped if i != '_' or o != '_') #stores the version of t without the underscores\n",
    "    return newin, newout"
   ]
  },
  {
   "cell_type": "raw",
   "id": "66fdecf8-6b09-4984-b858-c8676718b0e8",
   "metadata": {},
   "source": [
    "### Levenshtein Distance\n",
    "This function, levenshtein, computes the Levenshtein distance between two strings, s and t, using a recursive approach. The Levenshtein distance measures the minimum number of single-character edits (insertions, deletions, or substitutions) required to transform one string into another."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4be1718d-8e07-4e9d-b907-9567db6994bd",
   "metadata": {},
   "source": [
    "def levenshtein(s, t, inscost = 1.0, delcost = 1.0, substcost = 1.0):\n",
    "    \"\"\"Recursive implementation of Levenshtein, with alignments returned by\n",
    "    calculating the minimum number of edits (insert, delete, substitute) to transform s to t.\"\"\"\n",
    "    #s and t: input strings to compare\n",
    "    #inscost, delcost, substcost: costs of insertion, deletion, and substitution, respectively (default: 1.0)\n",
    "    @memolrec\n",
    "    #memoizer for lrec function (as seen below)\n",
    "    def lrec(spast, tpast, srem, trem, cost):\n",
    "        if len(srem) == 0:\n",
    "            #base case of recursive algorithm: if there are no more remaining characters in s\n",
    "            return spast + len(trem) * '_', tpast + trem, '', '', cost + len(trem)\n",
    "        if len(trem) == 0:\n",
    "            return spast + srem, tpast + len(srem) * '_', '', '', cost + len(srem)\n",
    "            #base case of recursive algorithm: if there are no more remaining characters in t\n",
    "\n",
    "        addcost = 0\n",
    "        #total cost of insertions/deletions/substitutions, initialized at 0\n",
    "        if srem[0] != trem[0]:\n",
    "            addcost = substcost\n",
    "            #checks if first characters of s and t are not equal, if so, add cost of substition to total cost\n",
    "\n",
    "        return min((lrec(spast + srem[0], tpast + trem[0], srem[1:], trem[1:], cost + addcost),\n",
    "                    #substition\n",
    "                    #adds first character of srem to spast, removes from srem\n",
    "                    #adds first character of trem to tpast, removes from trem\n",
    "                   lrec(spast + '_', tpast + trem[0], srem, trem[1:], cost + inscost),\n",
    "                    #insertion\n",
    "                    #adds underscore to spast to represent an insertion\n",
    "                    #add first character of trem to tpast, removes from trem\n",
    "                   lrec(spast + srem[0], tpast + '_', srem[1:], trem, cost + delcost)),\n",
    "                    #deletion\n",
    "                    #adds first character of srem to spast, removes from srem\n",
    "                    #adds underscore to tpast to represent an deletion\n",
    "                   key = lambda x: x[4])\n",
    "                    #finds the minimum cost using minimum number of substitutions, insertions, deletions\n",
    "\n",
    "    answer = lrec('', '', s, t, 0)\n",
    "    #spast and tpast are initialized as empty strings as no characters have been processed yet\n",
    "    #srem and trem are initialized as the full strings as they have yet to be processed\n",
    "    #cost initialized to 0\n",
    "    return answer[0],answer[1],answer[4]\n",
    "\n",
    "\n",
    "def memolrec(func):\n",
    "    \"\"\"Memoizer for Levenshtein (cache to check if Levenshtein distance between s and t has already been calculated\n",
    "    before to save time, as Levenshtein is a recursive algorithm that repeatedly solves subproblems.\"\"\"\n",
    "    cache = {}\n",
    "    @wraps(func)\n",
    "    def wrap(sp, tp, sr, tr, cost):\n",
    "        if (sr,tr) not in cache:\n",
    "            res = func(sp, tp, sr, tr, cost)\n",
    "            cache[(sr,tr)] = (res[0][len(sp):], res[1][len(tp):], res[4] - cost)\n",
    "        return sp + cache[(sr,tr)][0], tp + cache[(sr,tr)][1], '', '', cost + cache[(sr,tr)][2]\n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631e243-b5df-407a-ab72-b8662609c8da",
   "metadata": {
    "panel-layout": {
     "height": 95.3375015258789,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "### Alignment\n",
    "The alignprs function takes a lemma (base form of a word) and a form (inflected form) and aligns them to break each into three parts: a prefix, a stem, and a suffix. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a1e755c-8cb1-4282-8b9b-3518b2d282ed",
   "metadata": {},
   "source": [
    "def alignprs(lemma, form):\n",
    "    \"\"\"Align lemma and form into prefix, stem, and suffix.\"\"\"\n",
    "    alemma, aform = levenshtein('', '', lemma, form, 0)[:2]\n",
    "    lspace = max(len(alemma) - len(alemma.lstrip('_')), len(aform) - len(aform.lstrip('_')))\n",
    "    tspace = max(len(alemma[::-1]) - len(alemma[::-1].lstrip('_')), len(aform[::-1]) - len(aform[::-1].lstrip('_')))\n",
    "    return (alemma[:lspace], alemma[lspace:-tspace], alemma[-tspace:],\n",
    "            aform[:lspace], aform[lspace:-tspace], aform[-tspace:])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6aaa250f-429e-4775-a924-457935f054ae",
   "metadata": {},
   "source": [
    "### List of separable prefixes \n",
    "Defined a list of common German separable prefixes (SEPARABLE_PREFIXES), such as ab, auf, an, etc."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c721c2ca-51c1-4748-89a2-90b7c054c3a2",
   "metadata": {},
   "source": [
    "SEPARABLE_PREFIXES = {\"ab\", \"an\", \"auf\", \"aus\", \"bei\", \"ein\", \"mit\", \"nach\", \"vor\", \"weg\", \"zu\"} #EDIT"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c59b507-a61b-4005-8525-e9ebe93fadeb",
   "metadata": {},
   "source": [
    "### Extracting prefix and suffix rules\n",
    "This function analyzes how the prefix and suffix of a lemma change when it's transformed into an inflected form. The rules generated are for morphological processing, enabling a system to predict transformations for other words.\n",
    "### Tweaking rule extraction for prefixes\n",
    "Identifies if lemma contains a separable prefix, in order to disolate the prefix from the lemma for separate processing when generating rules."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b264a03d-55d3-4ac2-ad7e-7068af23de0c",
   "metadata": {},
   "source": [
    "def prefix_suffix_rules_get(lemma, form):\n",
    "    \"\"\"Extract a number of suffix-change and prefix-change rules\n",
    "    based on a given example lemma+inflected form. Analyze how the prefix and suffix change given a lemma and inflected form in order\n",
    "    to predict forms for other words.\"\"\" \n",
    "    prefix = ''\n",
    "    root = lemma\n",
    "    for p in SEPARABLE_PREFIXES:\n",
    "        if lemma.startswith(p):\n",
    "            prefix = p\n",
    "            root = lemma[len(p):] #removes prefix to isolate root\n",
    "            break\n",
    "    lp,lr,ls,fp,fr,fs = alignprs(root, form) #aligns root with lemma, get six parts, three for in three for out\n",
    "    #lp, lr, ls: lemma prefix, lemma root, lemma suffix\n",
    "    #fp, fr, fs: form prefix, form root, form suffix\n",
    "\n",
    "    #Prefix rules\n",
    "    prules = set()\n",
    "    if prefix:\n",
    "        prules.add((prefix, fp)) #rule for transforming the prefix\n",
    "\n",
    "    # Suffix rules\n",
    "    ins  = lr + ls + \">\"\n",
    "    #stem and suffix of lemma, '>' indicates end\n",
    "    outs = fr + fs + \">\"\n",
    "    #stem and suffix of form, '>' indicates end\n",
    "    srules = set()\n",
    "    #initializes empty set for suffix rules\n",
    "    for i in range(min(len(ins), len(outs))):\n",
    "        #iterates through either the stem+suffix of lemma or stem+suffix of form, whichever is shorter\n",
    "        srules.add((ins[i:], outs[i:]))\n",
    "        #adds ins and outs sliced from index i to end into set of suffix rules\n",
    "    srules = {(x[0].replace('_',''), x[1].replace('_','')) for x in srules}\n",
    "    #removes underscores from suffix rules by replacing with empty strings\n",
    "\n",
    "    return prules, srules\n",
    "    #returns set of prefix and suffix rules, respectively"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a90a41d9-d47d-48db-8ba7-02306b1ed1dc",
   "metadata": {},
   "source": [
    "### Applying Rules\n",
    "The apply_best_rule function applies the best-fitting prefix and suffix transformation rules to a given lemma, using morphological and syntactic descriptions (MSD) to guide the selection.\n",
    "\n",
    "### Adding logic for prefix rules\n",
    "We added logic to apply prefix rules that moved the prefix to the appropriate position (e.g., abhaken → haken ab), while keeping suffix rules the same."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7c27b6a-d6ef-481c-9b1b-8baf3815362e",
   "metadata": {},
   "source": [
    "def apply_best_rule(lemma, msd, allprules, allsrules):\n",
    "    \"\"\"Applies the longest-matching suffix-changing rule given an input\n",
    "    form and the MSD. Length ties in suffix rules are broken by frequency.\n",
    "    For prefix-changing rules, only the most frequent rule is chosen.\"\"\"\n",
    "    #lemma: base\n",
    "    #msd: morphosyntactic description (eg. past, plural, etc.)\n",
    "    #allprules: dictionary of prefix rules grouped by msd\n",
    "    #allsrules: dictionary of suffix rules grouped by msd\n",
    "\n",
    "    bestrulelen = 0\n",
    "    base = \"<\" + lemma + \">\"\n",
    "    #wrap lemma with '<' and '>' to match rule format\n",
    "    if msd not in allprules and msd not in allsrules:\n",
    "        return lemma #haven't seen this inflection, so bail out\n",
    "\n",
    "    if msd in allsrules:\n",
    "        applicablerules = [(x[0],x[1],y) for x,y in allsrules[msd].items() if x[0] in base]\n",
    "        if applicablerules:\n",
    "            bestrule = max(applicablerules, key = lambda x: (len(x[0]), x[2], len(x[1])))\n",
    "            base = base.replace(bestrule[0], bestrule[1])\n",
    "\n",
    "    if msd in allprules:\n",
    "        prefix = next((p for p in SEPARABLE_PREFIXES if base.startswith(\"<\" + p)), \"\")\n",
    "        if prefix:\n",
    "            applicablerules = [(x[0], x[1], y) for x, y in allprules[msd].items() if x[0] == prefix]\n",
    "            if applicablerules:\n",
    "                bestrule = max(applicablerules, key=lambda x: x[2])  # Choose most frequent rule\n",
    "                root = lemma[len(prefix):]  # Remove the original prefix\n",
    "                print(f\"Original Lemma: {lemma}, Prefix Detected: {prefix}\")\n",
    "                print(f\"Root: {root}\") #EDIT\n",
    "                lemma = f\"{bestrule[1]} {lemma}\"  # Add the transformed prefix at the correct position\n",
    "                #base = base.replace(bestrule[0], bestrule[1], 1)  # Replace prefix\n",
    "    \n",
    "    base = base.replace('<', '')\n",
    "    base = base.replace('>', '')\n",
    "    return base\n",
    "\n",
    "def numleadingsyms(s, symbol):\n",
    "    return len(s) - len(s.lstrip(symbol))\n",
    "    #returns count of leading occurrences of symbol\n",
    "\n",
    "\n",
    "def numtrailingsyms(s, symbol):\n",
    "    return len(s) - len(s.rstrip(symbol))\n",
    "    #returns count of trailing occurrences of symbol"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5d6708e-378c-42ff-976d-bbfac1bcba03",
   "metadata": {},
   "source": [
    "def main(argv):\n",
    "    options, remainder = getopt.gnu_getopt(argv[1:], 'ohp:', ['output','help','path='])\n",
    "    TEST, OUTPUT, HELP, path = False,False, False, '../data/'\n",
    "    for opt, arg in options:\n",
    "        if opt in ('-o', '--output'):\n",
    "            OUTPUT = True\n",
    "        if opt in ('-t', '--test'):\n",
    "            TEST = True\n",
    "        if opt in ('-h', '--help'):\n",
    "            HELP = True\n",
    "        if opt in ('-p', '--path'):\n",
    "            path = arg\n",
    "\n",
    "    if HELP:\n",
    "            print(\"\\n*** Baseline for the SIGMORPHON 2020 shared task ***\\n\")\n",
    "            print(\"By default, the program runs all languages only evaluating accuracy.\")\n",
    "            print(\"To create output files, use -o\")\n",
    "            print(\"The training and dev-data are assumed to live in ./part1/development_languages/\")\n",
    "            print(\"Options:\")\n",
    "            print(\" -o         create output files with guesses (and don't just evaluate)\")\n",
    "            print(\" -t         evaluate on test instead of dev\")\n",
    "            print(\" -p [path]  data files path. Default is ../data/\")\n",
    "            quit()\n",
    "\n",
    "    totalavg, numlang = 0.0, 0\n",
    "    for lang in [os.path.splitext(d)[0] for d in os.listdir(path) if '.trn' in d]:\n",
    "        allprules, allsrules = {}, {}\n",
    "        if not os.path.isfile(path + lang +  \".trn\"):\n",
    "            continue\n",
    "        lines = [line.strip() for line in open(path + lang + \".trn\", \"r\", encoding='utf8') if line != '\\n']\n",
    "\n",
    "        # First, test if language is predominantly suffixing or prefixing\n",
    "        # If prefixing, work with reversed strings\n",
    "        prefbias, suffbias = 0,0\n",
    "        for l in lines:\n",
    "            lemma, _, form = l.split(u'\\t')\n",
    "            aligned = halign(lemma, form)\n",
    "            if ' ' not in aligned[0] and ' ' not in aligned[1] and '-' not in aligned[0] and '-' not in aligned[1]:\n",
    "                prefbias += numleadingsyms(aligned[0],'_') + numleadingsyms(aligned[1],'_')\n",
    "                suffbias += numtrailingsyms(aligned[0],'_') + numtrailingsyms(aligned[1],'_')\n",
    "        for l in lines: # Read in lines and extract transformation rules from pairs\n",
    "            lemma, msd, form = l.split(u'\\t')\n",
    "            if prefbias > suffbias:\n",
    "                lemma = lemma[::-1]\n",
    "                form = form[::-1]\n",
    "            prules, srules = prefix_suffix_rules_get(lemma, form)\n",
    "\n",
    "            if msd not in allprules and len(prules) > 0:\n",
    "                allprules[msd] = {}\n",
    "            if msd not in allsrules and len(srules) > 0:\n",
    "                allsrules[msd] = {}\n",
    "\n",
    "            for r in prules:\n",
    "                if (r[0],r[1]) in allprules[msd]:\n",
    "                    allprules[msd][(r[0],r[1])] = allprules[msd][(r[0],r[1])] + 1\n",
    "                else:\n",
    "                    allprules[msd][(r[0],r[1])] = 1\n",
    "\n",
    "            for r in srules:\n",
    "                if (r[0],r[1]) in allsrules[msd]:\n",
    "                    allsrules[msd][(r[0],r[1])] = allsrules[msd][(r[0],r[1])] + 1\n",
    "                else:\n",
    "                    allsrules[msd][(r[0],r[1])] = 1\n",
    "\n",
    "        # Run eval on dev\n",
    "        devlines = [line.strip() for line in open(path + lang + \".dev\", \"r\", encoding='utf8') if line != '\\n']\n",
    "        if TEST:\n",
    "            devlines = [line.strip() for line in open(path + lang + \".tst\", \"r\", encoding='utf8') if line != '\\n']\n",
    "        numcorrect = 0\n",
    "        numguesses = 0\n",
    "        if OUTPUT:\n",
    "            outfile = open(path + lang + \".out\", \"w\", encoding='utf8')\n",
    "        for l in devlines:\n",
    "            lemma, msd, correct = l.split(u'\\t')\n",
    "#                    lemma, msd, = l.split(u'\\t')\n",
    "            if prefbias > suffbias:\n",
    "                lemma = lemma[::-1]\n",
    "            outform = apply_best_rule(lemma, msd, allprules, allsrules)\n",
    "            if prefbias > suffbias:\n",
    "                outform = outform[::-1]\n",
    "                lemma = lemma[::-1]\n",
    "            if outform == correct:\n",
    "                numcorrect += 1\n",
    "            numguesses += 1\n",
    "            if OUTPUT:\n",
    "                outfile.write(lemma + \"\\t\" + msd + \"\\t\" + outform + \"\\n\")\n",
    "\n",
    "        if OUTPUT:\n",
    "            outfile.close()\n",
    "\n",
    "        numlang += 1\n",
    "        totalavg += numcorrect/float(numguesses)\n",
    "\n",
    "        print(lang + \": \" + str(str(numcorrect/float(numguesses)))[0:7])\n",
    "\n",
    "    print(\"Average accuracy\", totalavg/float(numlang))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ef87e78-fb1a-4897-8856-f480ab5643e0",
   "metadata": {},
   "source": [
    "### Results\n",
    "| Dataset       | Accuracy Before Tweaks | Accuracy After Tweaks |\n",
    "|---------------|-------------------------|------------------------|\n",
    "| Training Set  | 93.75%                 | 97.93%                |\n",
    "| Development   | 74.5%                  | 76.3%                 |\n",
    "\n",
    "### Observations\n",
    "- Improvements in prefix handling significantly boosted accuracy in the training set, though less so in the dev set.\n",
    "- Prefixes were being correctly moved, but not being removed from the beginning of the verb\n",
    "\n",
    "- Handling separable verbs reduced common alignment errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "panel-cell-order": [
   "569b7055-4640-42f5-ba19-b9c4dd2be5fc",
   "fe12f7c1-bdb7-4d96-a1f6-d59e8619f56d",
   "b631e243-b5df-407a-ab72-b8662609c8da"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
